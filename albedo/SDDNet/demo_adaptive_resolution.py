import torch
import torchvision
from networks.sddnet import SDDNet
from PIL import Image
import numpy as np
from torchvision import transforms
import os
import torch.nn.functional as F

def process_demo_with_adaptive_resolution():
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # æ¨¡å‹é…ç½®
    ckpt_path = './ckpt/ep_019.ckpt'
    demo_dir = './demo'
    save_dir = './demo_results_adaptive'
    os.makedirs(save_dir, exist_ok=True)

    # åˆ›å»ºæ¨¡å‹
    model = SDDNet(backbone='efficientnet-b3',
                   proj_planes=16,
                   pred_planes=32,
                   use_pretrained=False,
                   fix_backbone=False,
                   has_se=False,
                   dropout_2d=0,
                   normalize=True,
                   mu_init=0.4,
                   reweight_mode='manual')

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    try:
        ckpt = torch.load(ckpt_path, map_location=device)
        model.load_state_dict(ckpt['model'], strict=False)
        print(f"Successfully loaded model from {ckpt_path}")
    except Exception as e:
        print(f"Error loading checkpoint: {e}")
        return

    model.to(device)
    model.eval()

    # å¤„ç†demoç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡
    img_files = [f for f in os.listdir(demo_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    if not img_files:
        print("No image files found in demo directory!")
        return

    # æµ‹è¯•ä¸åŒçš„åˆ†è¾¨ç‡ç­–ç•¥
    resolution_strategies = [
        {'name': 'original_512', 'size': 512, 'keep_aspect': False},
        {'name': 'keep_aspect_512', 'size': 512, 'keep_aspect': True},
        {'name': 'original_400', 'size': 400, 'keep_aspect': False},
        {'name': 'keep_aspect_400', 'size': 400, 'keep_aspect': True},
    ]

    def get_transform_with_strategy(strategy, image_size):
        if strategy['keep_aspect']:
            # ä¿æŒå®½é«˜æ¯”çš„resize
            w, h = image_size
            target_size = strategy['size']
            
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿å¾—é•¿è¾¹ä¸ºtarget_size
            scale = target_size / max(w, h)
            new_w, new_h = int(w * scale), int(h * scale)
            
            # ç¡®ä¿å°ºå¯¸æ˜¯32çš„å€æ•°ï¼ˆå¯¹äºå¤§å¤šæ•°CNNæ¶æ„æ›´å‹å¥½ï¼‰
            new_w = ((new_w + 31) // 32) * 32
            new_h = ((new_h + 31) // 32) * 32
            
            return transforms.Compose([
                transforms.Resize((new_h, new_w)),
                transforms.ToTensor()
            ]), (new_w, new_h)
        else:
            # ç›´æ¥resizeåˆ°æ­£æ–¹å½¢
            target_size = strategy['size']
            return transforms.Compose([
                transforms.Resize((target_size, target_size)),
                transforms.ToTensor()
            ]), (target_size, target_size)

    print(f"Processing {len(img_files)} image(s) with different resolution strategies...")

    with torch.no_grad():
        for img_file in img_files:
            print(f"\nProcessing {img_file}...")
            
            # è¯»å–å›¾åƒ
            img_path = os.path.join(demo_dir, img_file)
            image = Image.open(img_path).convert('RGB')
            original_size = image.size
            print(f"Original image size: {original_size}")
            
            base_name = os.path.splitext(img_file)[0]
            
            # æµ‹è¯•ä¸åŒçš„åˆ†è¾¨ç‡ç­–ç•¥
            for strategy in resolution_strategies:
                print(f"  Testing strategy: {strategy['name']}")
                
                # è·å–å¯¹åº”çš„transform
                img_transform, processed_size = get_transform_with_strategy(strategy, original_size)
                print(f"    Processed size: {processed_size}")
                
                # é¢„å¤„ç†
                img_tensor = img_transform(image).unsqueeze(0).to(device)
                
                # æ¨ç†
                result = model(img_tensor)
                
                # è·å–shadow mask
                shadow_logits = result['logit']
                shadow_mask = torch.sigmoid(shadow_logits).cpu()
                
                # è°ƒæ•´å›åŸå§‹å°ºå¯¸
                shadow_mask_resized = F.interpolate(shadow_mask, size=original_size[::-1], 
                                                  mode='bilinear', align_corners=False)
                
                # åº”ç”¨é˜ˆå€¼å¾—åˆ°äºŒå€¼mask
                binary_mask = (shadow_mask_resized > 0.5).float()
                
                # å‡†å¤‡å¯è§†åŒ–
                original_tensor = transforms.ToTensor()(image)
                
                # åˆ›å»ºå¯è§†åŒ–ç‰ˆæœ¬
                soft_mask_3ch = shadow_mask_resized[0].repeat(3, 1, 1)
                binary_mask_3ch = binary_mask[0].repeat(3, 1, 1)
                
                # åˆ›å»ºå åŠ æ•ˆæœ
                overlay = original_tensor.clone()
                mask_2d = binary_mask[0][0]
                alpha = 0.6
                
                # ä½¿ç”¨æ›´æ˜æ˜¾çš„é¢œè‰²æ ‡è®°é˜´å½±åŒºåŸŸ
                overlay[0] = torch.where(mask_2d > 0.5, 
                                       alpha * torch.tensor(1.0) + (1-alpha) * original_tensor[0], 
                                       original_tensor[0])
                overlay[1] = torch.where(mask_2d > 0.5, 
                                       (1-alpha) * original_tensor[1] * 0.5,  # å‡å°‘ç»¿è‰²
                                       original_tensor[1])
                overlay[2] = torch.where(mask_2d > 0.5, 
                                       (1-alpha) * original_tensor[2] * 0.5,  # å‡å°‘è“è‰²
                                       original_tensor[2])
                
                # ä¿å­˜ç»“æœ
                comparison_path = os.path.join(save_dir, f"{base_name}_{strategy['name']}_result.png")
                torchvision.utils.save_image([original_tensor, soft_mask_3ch, binary_mask_3ch, overlay], 
                                           comparison_path, nrow=4, padding=2)
                
                # å•ç‹¬ä¿å­˜mask
                mask_path = os.path.join(save_dir, f"{base_name}_{strategy['name']}_mask.png")
                torchvision.utils.save_image(binary_mask[0], mask_path)
                
                # ç»Ÿè®¡ä¿¡æ¯
                mask_ratio = torch.sum(binary_mask).item() / binary_mask.numel()
                print(f"    Shadow area ratio: {mask_ratio:.1%}")
                print(f"    Aspect ratio change: {original_size[0]/original_size[1]:.2f} -> {processed_size[0]/processed_size[1]:.2f}")

    print("\nâœ… Adaptive resolution processing completed!")
    print(f"ğŸ“ Results saved in: {save_dir}")
    print("\nğŸ“‹ Resolution strategies tested:")
    print("  - original_512: Direct resize to 512x512 (may distort aspect ratio)")
    print("  - keep_aspect_512: Keep aspect ratio, max dimension 512")
    print("  - original_400: Direct resize to 400x400 (may distort aspect ratio)")  
    print("  - keep_aspect_400: Keep aspect ratio, max dimension 400")
    print("\nğŸ’¡ Compare results to see which strategy works best for your images!")

if __name__ == "__main__":
    process_demo_with_adaptive_resolution()
